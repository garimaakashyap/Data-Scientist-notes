# Machine Learning Introduction - Supervised vs Unsupervised Learning & Regression

## Table of Contents
1. [What is Machine Learning?](#what-is-machine-learning)
2. [Supervised Learning](#supervised-learning)
3. [Unsupervised Learning](#unsupervised-learning)
4. [Supervised vs Unsupervised Comparison](#supervised-vs-unsupervised-comparison)
5. [Linear Regression](#linear-regression)
6. [Logistic Regression](#logistic-regression)

---

## What is Machine Learning?

**Machine Learning (ML)** is a subset of artificial intelligence where computers learn from data to make predictions or decisions without being explicitly programmed for every scenario.

### Traditional Programming vs Machine Learning

**Traditional Programming:**
```
Input Data + Rules → Output
Example: if age > 18 then "adult" else "minor"
```

**Machine Learning:**
```
Input Data + Output → Rules (Model)
Example: Learn patterns from thousands of examples
```

### Types of Machine Learning

1. **Supervised Learning** - Learning with labeled data
2. **Unsupervised Learning** - Learning from unlabeled data
3. **Reinforcement Learning** - Learning through rewards/punishments

---

## Supervised Learning

### Definition
**Supervised Learning** uses **labeled training data** to learn a function that maps inputs to outputs. We know the "correct answers" during training.

### Key Characteristics

✅ **Has labels (target variable)**
- Training data: (Features, Labels)
- Example: (Age, Income) → (Loan Approved: Yes/No)

✅ **Goal:** Learn to predict labels for new data

✅ **Can measure performance**
- Compare predictions to actual labels
- Calculate accuracy, error, etc.

### Types of Supervised Learning

#### 1. Classification
**Predicts categories/classes**

**Examples:**
- Email: Spam or Not Spam?
- Image: Cat, Dog, or Bird?
- Medical: Disease or Healthy?
- Loan: Approved or Rejected?

**Output:** Discrete categories

**Algorithms:**
- Logistic Regression
- Decision Trees
- Random Forest
- SVM
- KNN

#### 2. Regression
**Predicts continuous values**

**Examples:**
- House Price: $250,000
- Temperature: 25.5°C
- Sales: $1,500,000
- Age: 35 years

**Output:** Continuous numbers

**Algorithms:**
- Linear Regression
- Polynomial Regression
- Ridge/Lasso Regression
- Decision Trees (for regression)

### Supervised Learning Process

```
1. Collect labeled data
   Input (X): [Features]
   Output (y): [Labels]

2. Split data
   - Training set (70-80%)
   - Test set (20-30%)

3. Train model
   - Model learns patterns from training data
   - Finds relationship: X → y

4. Evaluate model
   - Test on unseen data
   - Measure accuracy/error

5. Make predictions
   - Use model on new data
```

### Example: Email Spam Detection

**Training Data:**
```
Email Text                    | Label
------------------------------|--------
"Win free money now!"         | Spam
"Meeting at 3pm tomorrow"    | Not Spam
"Click here for prizes!"     | Spam
"Project update attached"    | Not Spam
```

**Goal:** Learn to classify new emails as Spam or Not Spam

**After Training:**
```
New Email: "Get rich quick!" → Model predicts: Spam ✓
New Email: "Team meeting notes" → Model predicts: Not Spam ✓
```

### Python Example: Supervised Learning

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd

# Sample data: Predict if student passes exam
data = {
    'study_hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'attendance': [50, 60, 70, 80, 90, 85, 95, 100, 95, 100],
    'passed': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]  # Labels
}
df = pd.DataFrame(data)

# Features (X) and Labels (y)
X = df[['study_hours', 'attendance']]
y = df['passed']

# Split into training aVnd test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
model = LogisticRegression()
model.fit(X_train, y_train)  # Learning from labeled data

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")  # How well model learned
```

---

## Unsupervised Learning

### Definition
**Unsupervised Learning** finds **hidden patterns** in data **without labeled examples**. We don't know the "correct answers."

### Key Characteristics

✅ **No labels (only features)**
- Training data: (Features only)
- Example: (Age, Income, Spending) → ?

✅ **Goal:** Discover structure or patterns

✅ **Exploratory analysis**
- Find groups, reduce dimensions, find associations

### Types of Unsupervised Learning

#### 1. Clustering
**Groups similar data points together**

**Examples:**
- Customer Segmentation: Group 1, Group 2, Group 3
- Image Segmentation: Separate objects in image
- Document Clustering: Group similar documents

**Algorithms:**
- K-Means
- Hierarchical Clustering
- DBSCAN

#### 2. Dimensionality Reduction
**Reduces number of features while keeping information**

**Examples:**
- 100 features → 10 features
- Visualization (3D → 2D)
- Remove noise

**Algorithms:**
- PCA (Principal Component Analysis)
- t-SNE
- Autoencoders

#### 3. Association Rules
**Finds relationships between items**

**Examples:**
- Market Basket Analysis: "People who buy bread also buy butter"
- Recommendation: "Customers who liked X also liked Y"

**Algorithms:**
- Apriori
- FP-Growth

### Unsupervised Learning Process

```
1. Collect unlabeled data
   Input (X): [Features only]
   No Output (y)

2. Choose algorithm
   - Clustering: Find groups
   - Dimensionality Reduction: Reduce features
   - Association: Find relationships

3. Apply algorithm
   - Discover patterns
   - Group similar items
   - Reduce dimensions

4. Interpret results
   - What do clusters mean?
   - What patterns were found?
```

### Example: Customer Segmentation

**Data (No Labels):**
```
Customer | Age | Income | Spending
---------|-----|--------|----------
C1       | 25  | 30000  | 5000
C2       | 35  | 50000  | 12000
C3       | 45  | 70000  | 15000
C4       | 22  | 25000  | 3000
C5       | 50  | 80000  | 18000
```

**After Clustering:**
```
Group 1 (Young, Low Income): C1, C4
Group 2 (Middle, Medium Income): C2
Group 3 (Older, High Income): C3, C5
```

**Insight:** Three distinct customer segments discovered!

### Python Example: Unsupervised Learning

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Sample data: Customer behavior (no labels!)
data = {
    'age': [25, 35, 45, 22, 50, 30, 40],
    'income': [30000, 50000, 70000, 25000, 80000, 45000, 60000],
    'spending': [5000, 12000, 15000, 3000, 18000, 10000, 14000]
}
df = pd.DataFrame(data)

# Only features, no labels
X = df[['age', 'income', 'spending']]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply clustering (K-Means)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)  # No labels needed!

# Add cluster labels to data
df['cluster'] = clusters

print("Customer Segments Discovered:")
print(df.groupby('cluster').mean())
```

---

## Supervised vs Unsupervised Comparison

### Side-by-Side Comparison

| Aspect | Supervised Learning | Unsupervised Learning |
|--------|-------------------|---------------------|
| **Data** | Labeled (X, y) | Unlabeled (X only) |
| **Goal** | Predict target variable | Find hidden patterns |
| **Feedback** | Yes (we know answers) | No (no answers) |
| **Examples** | Classification, Regression | Clustering, Dimensionality Reduction |
| **Evaluation** | Accuracy, RMSE, Precision, Recall | Silhouette Score, Inertia, Explained Variance |
| **Use Case** | Predict future outcomes | Explore data, find structure |
| **Difficulty** | Easier to evaluate | Harder to evaluate |

### Visual Comparison

**Supervised Learning:**
```
Input: [Features] → Model → Output: [Known Label]
Example: [Age, Income] → Model → [Loan: Approved]
```

**Unsupervised Learning:**
```
Input: [Features] → Model → Output: [Patterns/Groups]
Example: [Age, Income, Spending] → Model → [Cluster 1, 2, 3]
```

### When to Use Which?

#### Use Supervised Learning When:
✅ You have labeled data
✅ You want to predict something specific
✅ You can measure performance
✅ You have clear target variable

**Examples:**
- Predict house prices
- Classify emails as spam
- Diagnose diseases
- Predict customer churn

#### Use Unsupervised Learning When:
✅ You don't have labels
✅ You want to explore data
✅ You want to find hidden patterns
✅ You want to reduce dimensions
✅ You want to discover groups

**Examples:**
- Customer segmentation
- Anomaly detection
- Feature reduction
- Market research
- Data visualization

### Real-World Example: E-commerce

**Supervised Learning:**
- **Problem:** Predict if customer will buy product
- **Data:** Customer features + Purchase history (labels)
- **Model:** Predicts "Will Buy" or "Won't Buy"

**Unsupervised Learning:**
- **Problem:** Find customer segments
- **Data:** Customer features only (no labels)
- **Model:** Discovers 3 customer groups (High Spenders, Bargain Hunters, Occasional Buyers)

---

## Linear Regression

### What is Linear Regression?

**Linear Regression** predicts a **continuous target variable** using a linear relationship with input features.

### Simple Example

**Problem:** Predict house price based on size

**Data:**
```
Size (sq ft) | Price ($)
-------------|----------
1000         | 200,000
1500         | 250,000
2000         | 300,000
2500         | 350,000
```

**Relationship:** As size increases, price increases linearly

### Simple Linear Regression

**Formula:**
```
y = mx + b

where:
y = predicted value (target)
x = input feature
m = slope (coefficient)
b = intercept
```

**Example:**
```
Price = 100 × Size + 50000

If Size = 2000:
Price = 100 × 2000 + 50000 = 250,000
```

### Multiple Linear Regression

**Formula:**
```
y = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ

where:
b₀ = intercept (baseline value)
b₁, b₂, ..., bₙ = coefficients (how much each feature affects target)
x₁, x₂, ..., xₙ = features
```

**Example: Predict house price**
```
Price = 50000 + 100×Size + 5000×Bedrooms + 2000×Age

If Size=2000, Bedrooms=3, Age=5:
Price = 50000 + 100×2000 + 5000×3 + 2000×5
     = 50000 + 200000 + 15000 + 10000
     = 275,000
```

### How Linear Regression Works

**Step 1: Training**
- Model finds best coefficients (b₀, b₁, ..., bₙ)
- Minimizes prediction error

**Step 2: Prediction**
- Use learned coefficients
- Calculate: y = b₀ + b₁x₁ + b₂x₂ + ...

### Cost Function: Mean Squared Error (MSE)

**Formula:**
```
MSE = (1/n) × Σ(yᵢ - ŷᵢ)²

where:
yᵢ = actual value
ŷᵢ = predicted value
n = number of samples
```

**Goal:** Minimize MSE (lower is better)

### Python Implementation

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np

# Sample data: House prices
data = {
    'size': [1000, 1500, 2000, 2500, 3000, 1800, 2200],
    'bedrooms': [2, 3, 3, 4, 4, 3, 3],
    'age': [5, 10, 15, 20, 25, 8, 12],
    'price': [200000, 250000, 300000, 350000, 400000, 280000, 320000]
}
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['size', 'bedrooms', 'age']]
y = df['price']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Model Coefficients:")
print(f"Intercept (b₀): ${model.intercept_:,.2f}")
for i, feature in enumerate(X.columns):
    print(f"{feature} (b{i+1}): ${model.coef_[i]:,.2f}")

print(f"\nModel Performance:")
print(f"MSE: {mse:,.2f}")
print(f"RMSE: ${rmse:,.2f}")
print(f"R² Score: {r2:.4f}")  # Closer to 1 is better

# Predict new house
new_house = pd.DataFrame({
    'size': [2000],
    'bedrooms': [3],
    'age': [10]
})
predicted_price = model.predict(new_house)
print(f"\nPredicted Price for New House: ${predicted_price[0]:,.2f}")
```

### Understanding Coefficients

**Example Output:**
```
Intercept (b₀): $50,000
size (b₁): $100
bedrooms (b₂): $5,000
age (b₃): -$2,000
```

**Interpretation:**
- **Intercept:** Base price = $50,000
- **Size:** Each sq ft adds $100
- **Bedrooms:** Each bedroom adds $5,000
- **Age:** Each year reduces price by $2,000 (negative = decreases value)

### Assumptions of Linear Regression

1. **Linearity:** Relationship between X and y is linear
2. **Independence:** Observations are independent
3. **Homoscedasticity:** Constant variance of errors
4. **Normality:** Errors are normally distributed

### Advantages
✅ Simple and interpretable
✅ Fast training and prediction
✅ Works well when relationship is linear
✅ No hyperparameters to tune
✅ Provides coefficients (feature importance)

### Disadvantages
❌ Assumes linear relationship
❌ Sensitive to outliers
❌ Can't capture complex patterns
❌ Requires feature scaling (sometimes)

### Use Cases
- House price prediction
- Sales forecasting
- Risk assessment
- Temperature prediction
- Stock price prediction (short-term)

### R² Score (Coefficient of Determination)

**Formula:**
```
R² = 1 - (SS_res / SS_tot)

where:
SS_res = Sum of squared residuals (errors)
SS_tot = Total sum of squares
```

**Interpretation:**
- **R² = 1.0:** Perfect predictions (all points on line)
- **R² = 0.8:** Model explains 80% of variance
- **R² = 0.0:** Model is no better than average
- **R² < 0:** Model is worse than average

**Example:**
```
R² = 0.85 means:
- Model explains 85% of price variation
- 15% is unexplained (random variation, other factors)
```

---

## Logistic Regression

### What is Logistic Regression?

**Logistic Regression** predicts **probabilities** for binary classification (yes/no, 0/1, spam/not spam).

### Key Difference from Linear Regression

| Linear Regression | Logistic Regression |
|------------------|-------------------|
| Predicts continuous values | Predicts probabilities (0-1) |
| Output: y = mx + b | Output: p = 1/(1 + e^(-z)) |
| Used for regression | Used for classification |

### Why Not Use Linear Regression for Classification?

**Problem:** Linear regression can output values outside 0-1
- Example: Predict probability = 1.5 or -0.3 (invalid!)

**Solution:** Use sigmoid function to constrain output to 0-1

### Sigmoid Function

**Formula:**
```
p = 1 / (1 + e^(-z))

where:
z = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ
```

**Properties:**
- Output always between 0 and 1
- S-shaped curve
- p = 0.5 when z = 0

**Visual:**
```
Probability
    1.0 |        ╱╲
        |      ╱    ╲
    0.5 |    ╱        ╲
        |  ╱            ╲
    0.0 |╱________________╲
        └──────────────────
              z (linear combination)
```

### Decision Threshold

**Default Threshold = 0.5**
- If p ≥ 0.5 → Predict class 1 (e.g., "Spam")
- If p < 0.5 → Predict class 0 (e.g., "Not Spam")

**Adjustable Threshold:**
- Lower threshold (0.3): More predictions of class 1
- Higher threshold (0.7): Fewer predictions of class 1
- Choose based on business needs (e.g., minimize false negatives)

### Cost Function: Log Loss (Cross-Entropy)

**Formula:**
```
Log Loss = -(1/n) × Σ[yᵢ × log(pᵢ) + (1-yᵢ) × log(1-pᵢ)]

where:
yᵢ = actual class (0 or 1)
pᵢ = predicted probability
```

**Goal:** Minimize Log Loss (lower is better)

### Python Implementation

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
import pandas as pd
import numpy as np

# Sample data: Predict if student passes exam
data = {
    'study_hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
    'attendance': [50, 60, 70, 80, 90, 85, 95, 100, 95, 100, 98, 100],
    'passed': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 0 = Failed, 1 = Passed
}
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['study_hours', 'attendance']]
y = df['passed']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)  # Probabilities

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(f"\nConfusion Matrix:")
print(f"                Predicted")
print(f"              No    Yes")
print(f"Actual No   {cm[0,0]:4d}  {cm[0,1]:4d}")
print(f"       Yes  {cm[1,0]:4d}  {cm[1,1]:4d}")

# Classification Report
print(f"\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC AUC Score (if binary classification)
if len(np.unique(y)) == 2:
    roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])
    print(f"ROC AUC Score: {roc_auc:.4f}")

# Model coefficients
print(f"\nModel Coefficients:")
print(f"Intercept (b₀): {model.intercept_[0]:.4f}")
for i, feature in enumerate(X.columns):
    print(f"{feature} (b{i+1}): {model.coef_[0][i]:.4f}")

# Predict probability for new student
new_student = pd.DataFrame({
    'study_hours': [6],
    'attendance': [85]
})
probability = model.predict_proba(new_student)[0]
print(f"\nNew Student Prediction:")
print(f"Probability of passing: {probability[1]:.2%}")
print(f"Predicted class: {'Pass' if model.predict(new_student)[0] == 1 else 'Fail'}")
```

### Understanding Coefficients

**Example Output:**
```
Intercept (b₀): -5.2341
study_hours (b₁): 0.4523
attendance (b₂): 0.1234
```

**Interpretation:**
- **Intercept:** Baseline log-odds
- **study_hours:** Each hour increases log-odds by 0.45
- **attendance:** Each % point increases log-odds by 0.12

**Odds Ratio:**
- e^(coefficient) = odds ratio
- e^(0.4523) = 1.57 means: 1 hour more study = 1.57x higher odds of passing

### Confusion Matrix

**Structure:**
```
                Predicted
              Negative  Positive
Actual Negative   TN      FP
       Positive   FN      TP
```

**Metrics:**
- **Accuracy:** (TP + TN) / Total
- **Precision:** TP / (TP + FP) - Of predicted positive, how many are actually positive?
- **Recall (Sensitivity):** TP / (TP + FN) - Of actual positive, how many did we catch?
- **F1-Score:** 2 × (Precision × Recall) / (Precision + Recall) - Balance of both

### Multi-class Classification

Logistic Regression can handle multiple classes:

**Methods:**
1. **One-vs-Rest (OvR):** Train one classifier per class
2. **Multinomial:** Direct multi-class extension

```python
# Multi-class example
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target

model = LogisticRegression(multi_class='multinomial')
model.fit(X, y)
predictions = model.predict(X)
```

### Advantages
✅ Provides probabilities, not just predictions
✅ Fast and efficient
✅ Interpretable (coefficients show feature importance)
✅ Works well for linearly separable data
✅ No hyperparameters to tune (usually)
✅ Handles multi-class problems

### Disadvantages
❌ Assumes linear decision boundary
❌ Requires large sample size
❌ Sensitive to outliers
❌ Can't capture complex non-linear patterns

### Use Cases
- Email spam detection
- Medical diagnosis (disease/no disease)
- Credit approval (approved/rejected)
- Customer churn prediction (churn/not churn)
- Fraud detection (fraud/legitimate)
- Marketing (click/won't click)

### When to Use Linear vs Logistic Regression

**Use Linear Regression when:**
- Target is continuous (price, temperature, age)
- You want to predict a number

**Use Logistic Regression when:**
- Target is categorical (yes/no, spam/not spam)
- You want probabilities
- Binary or multi-class classification

---

## Key Takeaways

### Supervised vs Unsupervised
1. **Supervised:** Has labels, predicts target variable
2. **Unsupervised:** No labels, finds patterns
3. **Classification:** Predict categories
4. **Regression:** Predict continuous values
5. **Clustering:** Group similar items
6. **Dimensionality Reduction:** Reduce features

### Linear Regression
1. Predicts continuous values
2. Formula: y = b₀ + b₁x₁ + b₂x₂ + ...
3. Minimizes MSE
4. Interpretable coefficients
5. Assumes linear relationship

### Logistic Regression
1. Predicts probabilities (0-1)
2. Uses sigmoid function
3. For binary/multi-class classification
4. Provides probabilities, not just predictions
5. Decision threshold = 0.5 (adjustable)

### Best Practices
1. **Always split data:** Train/Test sets
2. **Scale features:** For distance-based algorithms
3. **Evaluate properly:** Use appropriate metrics
4. **Start simple:** Linear/Logistic before complex models
5. **Understand your data:** Before choosing algorithm
6. **Feature engineering:** Often more important than algorithm

---

*Master these fundamentals before moving to advanced algorithms!*

