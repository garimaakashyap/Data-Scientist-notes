# Probability Distributions - Complete Guide

## Introduction
Probability distributions describe how likely different outcomes are. They're the foundation of statistics and data science. Understanding them helps you model real-world phenomena and make predictions.

## 1. Basic Concepts

### What is a Probability Distribution?
A **probability distribution** tells us:
- What values a variable can take
- How likely each value is

### Key Terms

**Random Variable (X):**
- A variable whose value is determined by chance
- Example: Number of heads when flipping a coin 10 times

**Probability Mass Function (PMF):**
- For **discrete** variables
- Gives probability of each specific value
- Example: P(X = 3) = 0.25

**Probability Density Function (PDF):**
- For **continuous** variables
- Gives probability density (area under curve = probability)
- Example: P(2 < X < 3) = area under curve

**Cumulative Distribution Function (CDF):**
- Probability that X ≤ some value
- Example: P(X ≤ 5) = 0.75

### Discrete vs Continuous

**Discrete:**
- Countable outcomes (0, 1, 2, 3, ...)
- Examples: Number of heads, number of customers

**Continuous:**
- Infinite possible values
- Examples: Height, weight, time

## 2. Discrete Distributions

### 2.1. Bernoulli Distribution

**What it models:** Single trial with two outcomes (success/failure)

**Parameters:**
- **p** = probability of success

**Example:** Flipping a coin once
- Success (heads): p = 0.5
- Failure (tails): 1 - p = 0.5

**PMF:**
```
P(X = 1) = p      (success)
P(X = 0) = 1 - p  (failure)
```

**Mean:** μ = p
**Variance:** σ² = p(1 - p)

**Python Code:**
```python
from scipy.stats import bernoulli
import numpy as np

# Create Bernoulli distribution (p = 0.6)
p = 0.6
dist = bernoulli(p)

# Probability of success
prob_success = dist.pmf(1)
print(f"P(X = 1) = {prob_success}")  # 0.6

# Probability of failure
prob_failure = dist.pmf(0)
print(f"P(X = 0) = {prob_failure}")  # 0.4

# Generate random samples
samples = dist.rvs(size=1000)
print(f"Mean of samples: {np.mean(samples):.2f}")  # ≈ 0.6
```

### 2.2. Binomial Distribution

**What it models:** Number of successes in n independent trials

**Parameters:**
- **n** = number of trials
- **p** = probability of success per trial

**Example:** Flipping a coin 10 times, count number of heads
- n = 10
- p = 0.5

**PMF:**
```
P(X = k) = C(n,k) × pᵏ × (1-p)ⁿ⁻ᵏ

where C(n,k) = n! / (k!(n-k)!)  (combinations)
```

**Mean:** μ = np
**Variance:** σ² = np(1-p)

**Example Calculation:**
Probability of getting exactly 7 heads in 10 flips:
```
P(X = 7) = C(10,7) × (0.5)⁷ × (0.5)³
         = 120 × 0.0078125 × 0.125
         = 0.117
```

**Python Code:**
```python
from scipy.stats import binom
import numpy as np
import matplotlib.pyplot as plt

# Parameters
n = 10  # number of trials
p = 0.5  # probability of success

# Create binomial distribution
dist = binom(n, p)

# Probability of exactly 7 successes
prob_7 = dist.pmf(7)
print(f"P(X = 7) = {prob_7:.4f}")  # 0.1172

# Probability of 7 or fewer successes
prob_7_or_less = dist.cdf(7)
print(f"P(X ≤ 7) = {prob_7_or_less:.4f}")  # 0.9453

# Probability of more than 7 successes
prob_more_than_7 = 1 - dist.cdf(7)
print(f"P(X > 7) = {prob_more_than_7:.4f}")  # 0.0547

# Visualize PMF
k_values = np.arange(0, n+1)
pmf_values = dist.pmf(k_values)

plt.bar(k_values, pmf_values)
plt.xlabel('Number of Successes')
plt.ylabel('Probability')
plt.title('Binomial Distribution (n=10, p=0.5)')
plt.show()
```

**Real-World Example:**
- **Problem:** 20% of products are defective. What's probability of exactly 3 defectives in a batch of 10?
- **Solution:** Binomial(n=10, p=0.2)
- **Answer:** P(X=3) ≈ 0.201

### 2.3. Poisson Distribution

**What it models:** Number of events in a fixed time/space interval

**Parameters:**
- **λ (lambda)** = average rate of events

**Example:** Number of customers arriving per hour
- λ = 5 customers/hour

**PMF:**
```
P(X = k) = (λᵏ × e⁻λ) / k!
```

**Mean:** μ = λ
**Variance:** σ² = λ

**Key Property:** Mean = Variance

**Python Code:**
```python
from scipy.stats import poisson
import numpy as np

# Average rate
lambda_param = 5

# Create Poisson distribution
dist = poisson(lambda_param)

# Probability of exactly 3 events
prob_3 = dist.pmf(3)
print(f"P(X = 3) = {prob_3:.4f}")  # 0.1404

# Probability of 3 or fewer events
prob_3_or_less = dist.cdf(3)
print(f"P(X ≤ 3) = {prob_3_or_less:.4f}")  # 0.2650

# Mean and variance (both equal to lambda)
print(f"Mean: {dist.mean()}")      # 5.0
print(f"Variance: {dist.var()}")    # 5.0
```

**Real-World Examples:**
- Calls to call center per hour
- Accidents per day
- Emails received per day
- Defects per square meter

### 2.4. Geometric Distribution

**What it models:** Number of trials until first success

**Parameters:**
- **p** = probability of success per trial

**Example:** Number of coin flips until first heads
- p = 0.5

**PMF:**
```
P(X = k) = (1-p)ᵏ⁻¹ × p
```

**Mean:** μ = 1/p
**Variance:** σ² = (1-p)/p²

**Python Code:**
```python
from scipy.stats import geom

# Probability of success
p = 0.3

# Create geometric distribution
dist = geom(p)

# Probability of first success on 3rd trial
prob_3 = dist.pmf(3)
print(f"P(X = 3) = {prob_3:.4f}")  # 0.147

# Expected number of trials
expected = dist.mean()
print(f"Expected trials: {expected:.2f}")  # 3.33
```

**Real-World Example:**
- Number of job applications until first offer
- Number of attempts until passing a test

## 3. Continuous Distributions

### 3.1. Normal Distribution (Gaussian)

**What it models:** Many natural phenomena (heights, test scores, measurement errors)

**Parameters:**
- **μ (mu)** = mean
- **σ (sigma)** = standard deviation

**PDF:**
```
f(x) = (1 / (σ√(2π))) × e^(-½((x-μ)/σ)²)
```

**Properties:**
- Bell-shaped, symmetric
- Mean = median = mode
- 68-95-99.7 rule applies

**68-95-99.7 Rule:**
- 68% within 1σ of mean
- 95% within 2σ of mean
- 99.7% within 3σ of mean

**Python Code:**
```python
from scipy.stats import norm
import numpy as np
import matplotlib.pyplot as plt

# Parameters
mu = 100  # mean
sigma = 15  # standard deviation

# Create normal distribution
dist = norm(mu, sigma)

# Probability between 85 and 115 (within 1 standard deviation)
prob = dist.cdf(115) - dist.cdf(85)
print(f"P(85 < X < 115) = {prob:.4f}")  # ≈ 0.6827 (68%)

# Probability of value greater than 130
prob_greater = 1 - dist.cdf(130)
print(f"P(X > 130) = {prob_greater:.4f}")  # ≈ 0.0228

# Percentile (95th percentile)
percentile_95 = dist.ppf(0.95)
print(f"95th percentile: {percentile_95:.2f}")  # ≈ 124.67

# Generate random samples
samples = dist.rvs(size=1000)
print(f"Sample mean: {np.mean(samples):.2f}")
print(f"Sample std: {np.std(samples):.2f}")

# Visualize
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
pdf_values = dist.pdf(x)

plt.plot(x, pdf_values)
plt.xlabel('Value')
plt.ylabel('Probability Density')
plt.title('Normal Distribution (μ=100, σ=15)')
plt.show()
```

**Standard Normal Distribution:**
- μ = 0, σ = 1
- Any normal distribution can be converted to standard normal:
  ```
  z = (x - μ) / σ
  ```

**Real-World Examples:**
- Human heights
- Test scores (IQ, SAT)
- Measurement errors
- Stock returns (approximately)

### 3.2. Uniform Distribution

**What it models:** All outcomes equally likely

**Parameters:**
- **a** = minimum value
- **b** = maximum value

**PDF:**
```
f(x) = 1 / (b - a)  for a ≤ x ≤ b
f(x) = 0            otherwise
```

**Mean:** μ = (a + b) / 2
**Variance:** σ² = (b - a)² / 12

**Python Code:**
```python
from scipy.stats import uniform

# Parameters
a = 0  # minimum
b = 10  # maximum

# Create uniform distribution
dist = uniform(a, b - a)  # Note: scipy uses loc and scale

# Probability between 3 and 7
prob = dist.cdf(7) - dist.cdf(3)
print(f"P(3 < X < 7) = {prob:.4f}")  # 0.4

# Mean
print(f"Mean: {dist.mean()}")  # 5.0
```

**Real-World Examples:**
- Rolling a fair die
- Random number generation
- Waiting time (sometimes)

### 3.3. Exponential Distribution

**What it models:** Time until next event (waiting times)

**Parameters:**
- **λ (lambda)** = rate parameter

**PDF:**
```
f(x) = λ × e^(-λx)  for x ≥ 0
```

**Mean:** μ = 1/λ
**Variance:** σ² = 1/λ²

**Memoryless Property:**
- P(X > s + t | X > s) = P(X > t)
- "Future doesn't depend on past"

**Python Code:**
```python
from scipy.stats import expon

# Rate parameter (events per unit time)
lambda_param = 0.5  # 0.5 events per hour

# Create exponential distribution
dist = expon(scale=1/lambda_param)  # scale = 1/λ

# Probability of waiting less than 2 hours
prob = dist.cdf(2)
print(f"P(X < 2) = {prob:.4f}")  # ≈ 0.6321

# Mean waiting time
mean_time = dist.mean()
print(f"Mean waiting time: {mean_time:.2f} hours")  # 2.0 hours
```

**Real-World Examples:**
- Time until next customer arrives
- Time until machine breaks down
- Time until radioactive decay

### 3.4. Student's T-Distribution

**What it models:** Sample means when population variance is unknown

**Parameters:**
- **ν (nu)** = degrees of freedom

**Properties:**
- Similar to normal but heavier tails
- As ν → ∞, approaches normal distribution
- Used in t-tests

**Python Code:**
```python
from scipy.stats import t

# Degrees of freedom
df = 10

# Create t-distribution
dist = t(df)

# Compare to normal
normal_dist = norm(0, 1)

# Probability of |t| > 2
prob_t = 2 * (1 - dist.cdf(2))
prob_normal = 2 * (1 - normal_dist.cdf(2))

print(f"P(|t| > 2) = {prob_t:.4f}")      # Higher
print(f"P(|z| > 2) = {prob_normal:.4f}")  # Lower
```

## 4. Distribution Relationships

### Central Limit Theorem

**Statement:** As sample size increases, sample means approach normal distribution

**Implication:** Even if data isn't normal, means of large samples are approximately normal

**Example:**
```python
import numpy as np
import matplotlib.pyplot as plt

# Non-normal distribution (exponential)
population = np.random.exponential(2, 10000)

# Sample means (n=30)
sample_means = []
for _ in range(1000):
    sample = np.random.choice(population, size=30)
    sample_means.append(np.mean(sample))

# Sample means are approximately normal!
plt.hist(sample_means, bins=50, density=True)
plt.title('Distribution of Sample Means (CLT)')
plt.show()
```

### Binomial → Normal

**When n is large and p is not extreme:**
- Binomial(n, p) ≈ Normal(np, np(1-p))

**Rule of thumb:** Use normal approximation when np > 5 and n(1-p) > 5

### Poisson → Normal

**When λ is large:**
- Poisson(λ) ≈ Normal(λ, √λ)

**Rule of thumb:** Use normal approximation when λ > 10

## 5. Choosing the Right Distribution

### Decision Tree

```
Is the variable discrete or continuous?
│
├─ Discrete:
│  │
│  ├─ Fixed number of trials?
│  │  ├─ Yes → Binomial
│  │  └─ No → Poisson
│  │
│  └─ Number of trials until success?
│     └─ Geometric
│
└─ Continuous:
   │
   ├─ Symmetric, bell-shaped?
   │  └─ Normal
   │
   ├─ All values equally likely?
   │  └─ Uniform
   │
   └─ Waiting time until event?
      └─ Exponential
```

### Common Applications

| Distribution | Use When |
|--------------|----------|
| **Normal** | Heights, test scores, measurement errors |
| **Binomial** | Number of successes in n trials |
| **Poisson** | Rare events in time/space |
| **Exponential** | Waiting times, lifetimes |
| **Uniform** | Random selection, no preference |
| **Geometric** | Trials until first success |

## 6. Working with Distributions in Python

### Complete Example: Quality Control

**Problem:** 5% of products are defective. In a batch of 100, what's the probability of:
1. Exactly 5 defectives?
2. 5 or fewer defectives?
3. More than 10 defectives?

**Solution:**
```python
from scipy.stats import binom
import numpy as np

# Parameters
n = 100  # batch size
p = 0.05  # defect rate

# Create binomial distribution
dist = binom(n, p)

# 1. Exactly 5 defectives
prob_exactly_5 = dist.pmf(5)
print(f"P(X = 5) = {prob_exactly_5:.4f}")

# 2. 5 or fewer
prob_5_or_less = dist.cdf(5)
print(f"P(X ≤ 5) = {prob_5_or_less:.4f}")

# 3. More than 10
prob_more_than_10 = 1 - dist.cdf(10)
print(f"P(X > 10) = {prob_more_than_10:.4f}")

# Expected number of defectives
expected = dist.mean()
print(f"Expected defectives: {expected:.2f}")  # 5.0
```

### Fitting Distributions to Data

```python
from scipy.stats import norm, expon
import numpy as np

# Generate sample data
data = np.random.normal(100, 15, 1000)

# Fit normal distribution
mu_est, sigma_est = norm.fit(data)
print(f"Estimated μ: {mu_est:.2f}")
print(f"Estimated σ: {sigma_est:.2f}")

# Compare to true values
print(f"True μ: 100.00")
print(f"True σ: 15.00")
```

## 7. Key Formulas Summary

### Discrete Distributions

| Distribution | PMF | Mean | Variance |
|--------------|-----|------|----------|
| **Bernoulli(p)** | p if x=1, (1-p) if x=0 | p | p(1-p) |
| **Binomial(n,p)** | C(n,k)pᵏ(1-p)ⁿ⁻ᵏ | np | np(1-p) |
| **Poisson(λ)** | (λᵏe⁻λ)/k! | λ | λ |
| **Geometric(p)** | (1-p)ᵏ⁻¹p | 1/p | (1-p)/p² |

### Continuous Distributions

| Distribution | PDF | Mean | Variance |
|--------------|-----|------|----------|
| **Normal(μ,σ)** | (1/(σ√(2π)))e^(-½((x-μ)/σ)²) | μ | σ² |
| **Uniform(a,b)** | 1/(b-a) | (a+b)/2 | (b-a)²/12 |
| **Exponential(λ)** | λe^(-λx) | 1/λ | 1/λ² |

## Key Takeaways

1. **Discrete distributions** model countable outcomes
2. **Continuous distributions** model measurable quantities
3. **Normal distribution** is most important (CLT)
4. **Binomial** for success/failure counts
5. **Poisson** for rare events
6. **Exponential** for waiting times
7. **68-95-99.7 rule** applies to normal distributions
8. **Central Limit Theorem** makes normal distribution fundamental
9. **Choose distribution** based on what you're modeling
10. **Python scipy.stats** has all common distributions

## Practice Problems

### Problem 1: Coin Flipping
Flip a fair coin 20 times. What's probability of exactly 10 heads?

**Answer:** Binomial(n=20, p=0.5), P(X=10) ≈ 0.176

### Problem 2: Customer Arrivals
Average 3 customers per hour. What's probability of 5 customers in next hour?

**Answer:** Poisson(λ=3), P(X=5) ≈ 0.101

### Problem 3: Test Scores
Scores are normally distributed with μ=75, σ=10. What's probability of scoring above 85?

**Answer:** Normal(75, 10), P(X>85) = 1 - Φ(1) ≈ 0.159

