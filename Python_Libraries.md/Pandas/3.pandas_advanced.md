# Pandas Advanced - Deep Dive

## 1. Advanced Data Reading

### Reading Different File Types
```python
# CSV with custom options
df = pd.read_csv('data.csv', 
                 sep=',',           # Separator
                 header=0,          # Header row
                 index_col=0,       # Use first column as index
                 na_values=['NA', 'N/A'],  # Custom NA values
                 skiprows=[0, 2],   # Skip specific rows
                 nrows=1000)        # Read only first 1000 rows

# Excel with multiple sheets
excel_file = pd.ExcelFile('data.xlsx')
df1 = pd.read_excel(excel_file, sheet_name='Sheet1')
df2 = pd.read_excel(excel_file, sheet_name='Sheet2')

# Read all sheets at once
all_sheets = pd.read_excel('data.xlsx', sheet_name=None)  # Returns dict

# JSON
df = pd.read_json('data.json', orient='records')

# HTML tables
tables = pd.read_html('https://example.com/table.html')

# SQL Database
import sqlite3
conn = sqlite3.connect('database.db')
df = pd.read_sql_query("SELECT * FROM table_name", conn)
```

### Chunking Large Files
```python
# Read large file in chunks
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = chunk[chunk['Value'] > 100]
    chunks.append(processed)

# Combine chunks
df = pd.concat(chunks, ignore_index=True)
```

## 2. Advanced Indexing with MultiIndex

### Creating MultiIndex
```python
# From arrays
arrays = [['A', 'A', 'B', 'B'], [1, 2, 1, 2]]
index = pd.MultiIndex.from_arrays(arrays, names=['First', 'Second'])

# From tuples
tuples = [('A', 1), ('A', 2), ('B', 1), ('B', 2)]
index = pd.MultiIndex.from_tuples(tuples, names=['First', 'Second'])

# From product
index = pd.MultiIndex.from_product([['A', 'B'], [1, 2]])

df = pd.DataFrame({'Value': [10, 20, 30, 40]}, index=index)
```

### Working with MultiIndex
```python
# Select by first level
print(df.loc['A'])

# Select by both levels
print(df.loc[('A', 1)])

# Cross-section
print(df.xs('A', level='First'))
print(df.xs(1, level='Second'))

# Swap levels
df_swapped = df.swaplevel('First', 'Second')

# Stack and unstack
df_stacked = df.stack()
df_unstacked = df.unstack()
```

## 3. Advanced Merging Strategies

### Merge with Different Column Names
```python
df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['A', 'B', 'C']})
df2 = pd.DataFrame({'Emp_ID': [1, 2, 4], 'Salary': [50, 60, 70]})

# Merge with different column names
merged = pd.merge(df1, df2, left_on='ID', right_on='Emp_ID')
```

### Merge on Index
```python
df1 = df1.set_index('ID')
df2 = df2.set_index('Emp_ID')

# Merge on index
merged = pd.merge(df1, df2, left_index=True, right_index=True)
# Or simply
merged = df1.join(df2)
```

### Merge with Suffixes
```python
df1 = pd.DataFrame({'ID': [1, 2], 'Value': [10, 20]})
df2 = pd.DataFrame({'ID': [1, 2], 'Value': [30, 40]})

# Add suffixes when columns overlap
merged = pd.merge(df1, df2, on='ID', suffixes=('_left', '_right'))
```

## 4. Advanced GroupBy Operations

### Named Aggregations
```python
df = pd.DataFrame({
    'Dept': ['IT', 'IT', 'HR', 'HR'],
    'Salary': [50000, 60000, 45000, 50000],
    'Age': [25, 30, 28, 32]
})

# Named aggregations (cleaner syntax)
result = df.groupby('Dept').agg(
    avg_salary=('Salary', 'mean'),
    total_salary=('Salary', 'sum'),
    avg_age=('Age', 'mean')
)
```

### Apply Custom Functions
```python
def custom_agg(group):
    return pd.Series({
        'salary_range': group['Salary'].max() - group['Salary'].min(),
        'employee_count': len(group),
        'salary_std': group['Salary'].std()
    })

result = df.groupby('Dept').apply(custom_agg)
```

### GroupBy with Transform and Filter
```python
# Transform: return same shape as input
df['Dept_Mean'] = df.groupby('Dept')['Salary'].transform('mean')

# Filter: filter groups based on condition
filtered = df.groupby('Dept').filter(lambda x: x['Salary'].mean() > 50000)
```

## 5. Time Series Advanced Operations

### Time Zone Handling
```python
# Create timezone-aware datetime
dates = pd.date_range('2024-01-01', periods=5, freq='D', tz='UTC')
df = pd.DataFrame({'Value': [10, 20, 30, 40, 50]}, index=dates)

# Convert timezone
df_est = df.tz_convert('US/Eastern')

# Localize (add timezone)
df_naive = pd.DataFrame({'Value': [10, 20]}, 
                       index=pd.date_range('2024-01-01', periods=2))
df_aware = df_naive.tz_localize('UTC')
```

### Resampling and Frequency Conversion
```python
# Create sample data
dates = pd.date_range('2024-01-01', periods=100, freq='H')
df = pd.DataFrame({'Value': range(100)}, index=dates)

# Downsample (higher to lower frequency)
daily = df.resample('D').sum()        # Hourly to daily
weekly = df.resample('W').mean()      # Daily to weekly

# Upsample (lower to higher frequency)
hourly = daily.resample('H').ffill()  # Forward fill
hourly = daily.resample('H').interpolate()  # Interpolate

# Custom frequency
custom = df.resample('2H').sum()      # Every 2 hours
```

### Time-based Grouping
```python
df = pd.DataFrame({
    'Date': pd.date_range('2024-01-01', periods=100, freq='D'),
    'Value': range(100)
})
df = df.set_index('Date')

# Group by year
yearly = df.groupby(df.index.year).sum()

# Group by month
monthly = df.groupby(df.index.month).mean()

# Group by weekday
weekday = df.groupby(df.index.day_name()).mean()
```

## 6. Advanced Window Functions

### Custom Window Functions
```python
df = pd.DataFrame({'Value': range(1, 11)})

# Rolling with custom window
df['Rolling_Mean'] = df['Value'].rolling(window=3, center=True).mean()

# Rolling with minimum periods
df['Rolling_Mean'] = df['Value'].rolling(window=5, min_periods=2).mean()

# Weighted rolling
weights = [0.1, 0.3, 0.6]
df['Weighted'] = df['Value'].rolling(window=3).apply(
    lambda x: (x * weights).sum()
)

# Expanding with custom function
df['Expanding_Sum'] = df['Value'].expanding().sum()
df['Expanding_Max'] = df['Value'].expanding().max()
```

### EWM (Exponentially Weighted Moving)
```python
# Exponentially weighted moving average
df['EWM'] = df['Value'].ewm(span=3).mean()

# Exponentially weighted moving std
df['EWM_Std'] = df['Value'].ewm(span=3).std()
```

## 7. Performance Optimization

### Using Categorical for Memory
```python
# Convert string columns to categorical
df['Category'] = df['Category'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))

# Optimize integer types
df['ID'] = df['ID'].astype('int32')  # Instead of int64
```

### Vectorization
```python
# Slow: Using apply with loops
df['Result'] = df.apply(lambda row: row['A'] + row['B'], axis=1)

# Fast: Vectorized operations
df['Result'] = df['A'] + df['B']
```

### Using Query for Speed
```python
# Faster filtering with query
result = df.query('Age > 30 and Salary > 50000')

# Instead of
result = df[(df['Age'] > 30) & (df['Salary'] > 50000)]
```

## 8. Advanced String Operations

### Regex Operations
```python
df = pd.DataFrame({
    'Text': ['abc123', 'def456', 'ghi789', 'jkl012']
})

# Extract numbers
df['Numbers'] = df['Text'].str.extract(r'(\d+)')

# Extract multiple groups
df[['Letters', 'Numbers']] = df['Text'].str.extract(r'([a-z]+)(\d+)')

# Replace with regex
df['Text'] = df['Text'].str.replace(r'\d+', 'XXX', regex=True)

# Find all matches
df['All_Numbers'] = df['Text'].str.findall(r'\d')
```

### String Methods Chaining
```python
df = pd.DataFrame({'Name': ['  alice smith  ', '  BOB JONES  ']})

# Chain multiple string operations
df['Clean'] = (df['Name']
               .str.strip()
               .str.lower()
               .str.title())
```

## 9. Custom Data Types

### Creating Custom Extension Types
```python
from pandas.api.extensions import ExtensionDtype, ExtensionArray

# Example: Custom email type (simplified)
class EmailType(ExtensionDtype):
    name = "email"
    
    @classmethod
    def construct_array_type(cls):
        return EmailArray
```

## 10. Advanced Pivot and Melt

### Pivot Table with Multiple Values
```python
df = pd.DataFrame({
    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02'],
    'Product': ['A', 'B', 'A', 'B'],
    'Sales': [100, 200, 150, 250],
    'Quantity': [10, 20, 15, 25]
})

# Pivot with multiple values
pivot = pd.pivot_table(df, 
                      values=['Sales', 'Quantity'],
                      index='Date',
                      columns='Product',
                      aggfunc='sum')
```

### Melt (Unpivot)
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob'],
    'Math': [90, 85],
    'Science': [95, 80],
    'English': [88, 90]
})

# Melt (wide to long format)
melted = pd.melt(df, 
                id_vars='Name',
                value_vars=['Math', 'Science', 'English'],
                var_name='Subject',
                value_name='Score')
```

## 11. Styling DataFrames

```python
# Style with colors
def highlight_max(s):
    is_max = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_max]

styled = df.style.apply(highlight_max)

# Conditional formatting
styled = df.style.background_gradient(cmap='viridis')

# Format numbers
styled = df.style.format({'Salary': '${:,.2f}', 'Age': '{:.0f}'})
```

## 12. Parallel Processing

### Using Dask for Large Data
```python
import dask.dataframe as dd

# Read large CSV in parallel
df = dd.read_csv('large_file.csv')

# Operations are lazy (computed when needed)
result = df.groupby('Category').sum()

# Compute result
result_computed = result.compute()
```

## 13. Advanced Missing Data Handling

### Interpolation Methods
```python
df = pd.DataFrame({'Value': [1, None, None, 4, 5, None, 7]})

# Linear interpolation
df['Linear'] = df['Value'].interpolate(method='linear')

# Polynomial interpolation
df['Poly'] = df['Value'].interpolate(method='polynomial', order=2)

# Time-based interpolation
df_time = df.set_index(pd.date_range('2024-01-01', periods=len(df)))
df_time['Time_Interp'] = df_time['Value'].interpolate(method='time')
```

### Forward Fill and Backward Fill
```python
# Forward fill (use previous value)
df['FFill'] = df['Value'].ffill()

# Backward fill (use next value)
df['BFill'] = df['Value'].bfill()

# Limit fill
df['FFill_Limited'] = df['Value'].ffill(limit=2)
```

## Key Takeaways
- Use chunking for very large files
- MultiIndex enables complex hierarchical data
- Time series operations are powerful for temporal data
- Vectorization is key for performance
- Advanced groupby operations enable complex aggregations
- Styling makes DataFrames presentation-ready
- Parallel processing helps with big data

