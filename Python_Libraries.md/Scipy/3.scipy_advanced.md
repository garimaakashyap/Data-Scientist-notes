# SciPy Advanced - Deep Dive

## 1. Advanced Optimization Techniques

### Multi-objective Optimization
```python
from scipy.optimize import minimize
import numpy as np

# Pareto optimization (weighted sum approach)
def objective(x, weights):
    f1 = x[0]**2 + x[1]**2  # First objective
    f2 = (x[0] - 1)**2 + (x[1] - 1)**2  # Second objective
    return weights[0] * f1 + weights[1] * f2

# Different weight combinations
weights_list = [[1, 0], [0.5, 0.5], [0, 1]]
for weights in weights_list:
    result = minimize(objective, x0=[0, 0], args=(weights,))
    print(f"Weights {weights}: x = {result.x}, f = {result.fun}")
```

### Basin Hopping (Global Optimization)
```python
from scipy.optimize import basinhopping

def objective(x):
    return (x[0] - 2)**2 + (x[1] - 3)**2 + np.sin(x[0]) * np.sin(x[1])

# Global optimization with basin hopping
result = basinhopping(objective, x0=[0, 0], niter=100)
print(f"Global minimum: x = {result.x}, f(x) = {result.fun}")
```

### Custom Optimizers
```python
from scipy.optimize import minimize
import numpy as np

def objective(x):
    return x[0]**2 + x[1]**2

# Try different optimization methods
methods = ['BFGS', 'L-BFGS-B', 'SLSQP', 'TNC']
for method in methods:
    result = minimize(objective, x0=[1, 1], method=method)
    print(f"{method}: x = {result.x}, iterations = {result.nit}")
```

## 2. Advanced Integration Methods

### Adaptive Integration
```python
from scipy import integrate
import numpy as np

def f(x):
    return np.sin(x) / x

# Adaptive quadrature with error control
result, error = integrate.quad(f, 0, np.inf, epsabs=1e-6, epsrel=1e-6)
print(f"Integral: {result:.6f}, Error: {error:.2e}")
```

### Monte Carlo Integration
```python
from scipy import integrate
import numpy as np

def f(x):
    return x**2

# Monte Carlo integration
def monte_carlo_integrate(f, a, b, n_samples=10000):
    x_random = np.random.uniform(a, b, n_samples)
    y_random = f(x_random)
    integral = (b - a) * np.mean(y_random)
    return integral

result = monte_carlo_integrate(f, 0, 2)
print(f"Monte Carlo integral: {result:.6f}")
```

### Vector-valued Integration
```python
from scipy import integrate
import numpy as np

def f(x):
    return np.array([x**2, x**3, np.sin(x)])

# Integrate vector-valued function
result, error = integrate.quad_vec(f, 0, 2)
print(f"Vector integral: {result}")
```

## 3. Advanced Interpolation Methods

### Radial Basis Function Interpolation
```python
from scipy.interpolate import RBFInterpolator
import numpy as np

# Scattered data points
points = np.random.rand(50, 2)
values = np.sin(points[:, 0] * np.pi) * np.cos(points[:, 1] * np.pi)

# Create RBF interpolator
rbf = RBFInterpolator(points, values, kernel='thin_plate_spline')

# Interpolate at new points
new_points = np.random.rand(20, 2)
new_values = rbf(new_points)
```

### Multivariate Spline Interpolation
```python
from scipy.interpolate import SmoothBivariateSpline
import numpy as np

# 2D data
x = np.random.rand(100)
y = np.random.rand(100)
z = np.sin(x * np.pi) * np.cos(y * np.pi)

# Create spline
spline = SmoothBivariateSpline(x, y, z, s=0.1)

# Evaluate
x_new = np.linspace(0, 1, 50)
y_new = np.linspace(0, 1, 50)
X_new, Y_new = np.meshgrid(x_new, y_new)
Z_new = spline(X_new, Y_new)
```

## 4. Advanced Statistical Methods

### Kernel Density Estimation
```python
from scipy.stats import gaussian_kde
import numpy as np

# Sample data
data = np.random.normal(0, 1, 1000)

# Create KDE
kde = gaussian_kde(data)

# Evaluate PDF
x = np.linspace(-5, 5, 100)
pdf = kde(x)

# Sample from KDE
samples = kde.resample(1000)
```

### Maximum Likelihood Estimation
```python
from scipy.stats import norm
from scipy.optimize import minimize
import numpy as np

# Generate data
data = np.random.normal(5, 2, 1000)

# Negative log-likelihood
def neg_log_likelihood(params, data):
    mu, sigma = params
    return -np.sum(norm.logpdf(data, loc=mu, scale=sigma))

# MLE
result = minimize(neg_log_likelihood, x0=[0, 1], args=(data,))
mu_mle, sigma_mle = result.x
print(f"MLE: μ = {mu_mle:.4f}, σ = {sigma_mle:.4f}")
```

### Bayesian Statistics
```python
from scipy.stats import beta, binom
import numpy as np

# Prior: Beta distribution
prior = beta(a=2, b=2)

# Likelihood: Binomial
n_trials = 100
n_successes = 60
likelihood = binom(n=n_trials, p=0.6)

# Posterior: Beta (conjugate prior)
posterior = beta(a=2 + n_successes, b=2 + n_trials - n_successes)

# Posterior mean
posterior_mean = posterior.mean()
print(f"Posterior mean: {posterior_mean:.4f}")
```

## 5. Advanced Linear Algebra

### Sparse Matrix Operations
```python
from scipy.sparse import csr_matrix, csc_matrix, diags
from scipy.sparse.linalg import eigs, svds
import numpy as np

# Create large sparse matrix
n = 1000
diagonal = np.ones(n)
off_diagonal = 0.5 * np.ones(n-1)
A = diags([diagonal, off_diagonal, off_diagonal], [0, 1, -1])

# Find largest eigenvalues
eigenvalues, eigenvectors = eigs(A, k=5, which='LM')
print(f"Largest eigenvalues: {eigenvalues}")

# SVD of sparse matrix
U, s, Vt = svds(A, k=5)
print(f"Singular values: {s}")
```

### Matrix Decompositions
```python
from scipy.linalg import lu, cholesky, schur
import numpy as np

A = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]])

# LU decomposition
P, L, U = lu(A)
print("L:\n", L)
print("U:\n", U)

# Cholesky decomposition (for positive definite)
L_chol = cholesky(A)
print("Cholesky L:\n", L_chol)

# Schur decomposition
T, Z = schur(A)
print("Schur form T:\n", T)
```

## 6. Advanced Signal Processing

### Wavelet Transform
```python
from scipy import signal
import numpy as np

# Create signal
t = np.linspace(0, 1, 1000)
signal_data = np.sin(2 * np.pi * 5 * t) + np.random.normal(0, 0.1, 1000)

# Continuous Wavelet Transform
widths = np.arange(1, 31)
cwt = signal.cwt(signal_data, signal.ricker, widths)
```

### Spectral Analysis
```python
from scipy import signal
import numpy as np

# Create signal
t = np.linspace(0, 1, 1000)
signal_data = np.sin(2 * np.pi * 5 * t) + 0.5 * np.sin(2 * np.pi * 10 * t)

# Power spectral density
frequencies, psd = signal.welch(signal_data, fs=1000, nperseg=256)
print(f"Peak frequencies: {frequencies[np.argmax(psd)]} Hz")

# Short-time Fourier transform
f, t_stft, Zxx = signal.stft(signal_data, fs=1000, nperseg=256)
```

### Adaptive Filtering
```python
from scipy import signal
import numpy as np

# LMS (Least Mean Squares) adaptive filter
def lms_filter(x, d, mu=0.01, filter_length=10):
    n = len(x)
    w = np.zeros(filter_length)
    y = np.zeros(n)
    e = np.zeros(n)
    
    for i in range(filter_length, n):
        x_vec = x[i-filter_length:i]
        y[i] = np.dot(w, x_vec)
        e[i] = d[i] - y[i]
        w = w + mu * e[i] * x_vec
    
    return y, e, w

# Example usage
x = np.random.randn(1000)
d = signal.lfilter([1, 0.5], [1], x) + 0.1 * np.random.randn(1000)
y, e, w = lms_filter(x, d)
```

## 7. Advanced Image Processing

### Image Segmentation
```python
from scipy import ndimage
from scipy.ndimage import label, find_objects
import numpy as np

# Create binary image
image = np.random.rand(100, 100) > 0.7

# Label connected components
labeled, num_features = label(image)
print(f"Found {num_features} connected components")

# Find bounding boxes
objects = find_objects(labeled)
for i, obj in enumerate(objects):
    print(f"Object {i+1}: {obj}")
```

### Image Registration
```python
from scipy.ndimage import shift, rotate
from scipy.optimize import minimize
import numpy as np

# Reference image
reference = np.random.rand(100, 100)

# Create shifted/rotated image
shifted = shift(reference, [5, 10])
rotated = rotate(shifted, 15)

# Registration function
def registration_error(params, reference, target):
    dx, dy, angle = params
    transformed = rotate(shift(reference, [dx, dy]), angle)
    return np.sum((transformed - target)**2)

# Find transformation
result = minimize(registration_error, x0=[0, 0, 0], 
                  args=(reference, rotated))
print(f"Estimated shift: {result.x[:2]}, rotation: {result.x[2]}")
```

## 8. Advanced Spatial Algorithms

### Convex Hull
```python
from scipy.spatial import ConvexHull
import numpy as np

# Random points
points = np.random.rand(50, 2)

# Compute convex hull
hull = ConvexHull(points)

# Get hull vertices
hull_points = points[hull.vertices]
print(f"Hull has {len(hull_points)} vertices")
```

### Voronoi Diagrams
```python
from scipy.spatial import Voronoi, voronoi_plot_2d
import numpy as np

# Generate points
points = np.random.rand(20, 2)

# Compute Voronoi diagram
vor = Voronoi(points)

# Access Voronoi regions
print(f"Number of regions: {len(vor.regions)}")
print(f"Number of vertices: {len(vor.vertices)}")
```

### Delaunay Triangulation
```python
from scipy.spatial import Delaunay
import numpy as np

# Points
points = np.random.rand(30, 2)

# Delaunay triangulation
tri = Delaunay(points)

# Access triangles
print(f"Number of triangles: {len(tri.simplices)}")

# Find triangle containing point
point = np.array([0.5, 0.5])
triangle_index = tri.find_simplex(point)
print(f"Point is in triangle: {triangle_index}")
```

## 9. Advanced Special Functions

### Hypergeometric Functions
```python
from scipy.special import hyp2f1, hyperu
import numpy as np

# Hypergeometric function 2F1
x = np.linspace(0, 0.9, 100)
result = hyp2f1(0.5, 0.5, 1.0, x)

# Confluent hypergeometric function
result = hyperu(1, 2, x)
```

### Elliptic Functions
```python
from scipy.special import ellipk, ellipe
import numpy as np

# Complete elliptic integrals
m = np.linspace(0, 0.99, 100)
K = ellipk(m)  # First kind
E = ellipe(m)  # Second kind
```

### Airy Functions
```python
from scipy.special import airy, airye
import numpy as np

x = np.linspace(-10, 10, 100)
Ai, Aip, Bi, Bip = airy(x)  # Airy functions
Ai_e, Aip_e, Bi_e, Bip_e = airye(x)  # Exponentially scaled
```

## 10. Differential Equations

### ODE Solving
```python
from scipy.integrate import odeint, solve_ivp
import numpy as np

# Define ODE: dy/dt = -y
def dydt(y, t):
    return -y

# Initial condition
y0 = 1.0
t = np.linspace(0, 10, 100)

# Solve ODE
solution = odeint(dydt, y0, t)

# Using solve_ivp (more modern)
def dydt_ivp(t, y):
    return -y

sol = solve_ivp(dydt_ivp, [0, 10], [y0], t_eval=t)
```

### System of ODEs
```python
from scipy.integrate import solve_ivp
import numpy as np

# Lotka-Volterra equations
def lotka_volterra(t, y, alpha, beta, gamma, delta):
    x, y_pop = y
    dxdt = alpha * x - beta * x * y_pop
    dydt = delta * x * y_pop - gamma * y_pop
    return [dxdt, dydt]

# Parameters
params = (1.0, 0.1, 1.5, 0.075)
y0 = [10, 5]
t_span = (0, 50)
t_eval = np.linspace(0, 50, 1000)

# Solve
sol = solve_ivp(lotka_volterra, t_span, y0, args=params, t_eval=t_eval)
```

### Partial Differential Equations
```python
from scipy.integrate import solve_ivp
import numpy as np

# Heat equation (simplified 1D)
def heat_equation(t, u, dx, alpha):
    dudt = np.zeros_like(u)
    dudt[1:-1] = alpha * (u[2:] - 2*u[1:-1] + u[:-2]) / dx**2
    return dudt

# Initial condition
x = np.linspace(0, 1, 50)
u0 = np.sin(np.pi * x)

# Solve
sol = solve_ivp(heat_equation, [0, 0.1], u0, 
                args=(x[1]-x[0], 0.1), t_eval=np.linspace(0, 0.1, 20))
```

## 11. Graph Algorithms

### Shortest Path
```python
from scipy.sparse.csgraph import dijkstra, shortest_path
import numpy as np

# Create graph (adjacency matrix)
graph = np.array([
    [0, 1, 0, 0],
    [1, 0, 1, 0],
    [0, 1, 0, 1],
    [0, 0, 1, 0]
])

# Shortest path from node 0
distances, predecessors = dijkstra(graph, indices=0, return_predecessors=True)
print(f"Distances from node 0: {distances}")
```

### Connected Components
```python
from scipy.sparse.csgraph import connected_components
import numpy as np

# Graph
graph = np.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 0, 1],
    [0, 0, 1, 0]
])

# Find connected components
n_components, labels = connected_components(graph, directed=False)
print(f"Number of components: {n_components}")
print(f"Component labels: {labels}")
```

## Key Takeaways
- Advanced optimization handles complex, multi-objective problems
- Monte Carlo methods solve difficult integration problems
- Advanced statistics enable sophisticated data analysis
- Sparse matrices are essential for large-scale computations
- Differential equation solvers model dynamic systems
- Graph algorithms analyze network structures
- SciPy provides professional-grade scientific computing tools

